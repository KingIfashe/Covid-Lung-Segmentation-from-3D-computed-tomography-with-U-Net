{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDS-M04: Deep Learning in Medical Imaging\n",
    "\n",
    "HealthDataScience CDT Users Guide (how to access VM) <br>\n",
    "[Setup instructions](https://canvas.ox.ac.uk/courses/151592/files/4750711?wrap=1)\n",
    "\n",
    "\n",
    "### Required packages\n",
    "\n",
    "[1] [matplotlib](http://matplotlib.org) can be used for plotting graphs in python\n",
    "\n",
    "[2] [pytorch](https://pytorch.org/docs/stable/index.html) is library widely used for bulding deep-learning frameworks\n",
    "\n",
    "[3] [Tensorboard](https://pytorch.org/docs/stable/tensorboard.html) is used to visualise your training and validation loss and accuracy development - It is important to observe it!!!\n",
    "\n",
    "[4] [TorchVision](https://pytorch.org/vision/stable/index.html) you can use available datasets that include one you are required to use in this tutorial (CIFAR10) and also use augmentations (meaning you can increase data variability for improved accuracy and model generalisation)\n",
    "\n",
    "[5] [Scikit-learn] Metrics\n",
    "\n",
    "**Reference:** \n",
    "\n",
    "[1] Original U-Net paper: https://arxiv.org/pdf/1505.04597.pdf\n",
    "\n",
    "[2] Link to presentation: https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/\n",
    "\n",
    "\n",
    "**What will you learn here?**\n",
    "\n",
    "- You wil learn how to load your custom data for semantic segmentation task\n",
    "\n",
    "- You will make an encoder-decoder U-Net architecture \n",
    "\n",
    "- You will learn how to group repeated blocks into one while designing U-Net\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/EtyQs.png\" style=\"width:800px;height:400px;\">\n",
    "<caption><center> <u>Figure</u>: U-Net architecture for image segmentation.</center></caption>\n",
    "\n",
    "**Concentrate on:**\n",
    "\n",
    "[1] Encoder layers (this is similar to your classification exercise)\n",
    "\n",
    "[2] Decoder layers (upscaling of images occur here as you want to obtain per-pixel segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "# always check your version\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New : We will setup a seed for reproducibility (ask tutor if you do not understand it)\n",
    "- Setting a seed helps to regenerate the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_seed= 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and pre-processing\n",
    "**Steps**\n",
    "\n",
    "[1] Load data - data is provided already available for you to use in ``/cdtshared/Cohort-2021/HDS-M05/segmentation/MontgomerySet``\n",
    "- Images are in folder ``/cdtshared/Cohort-2021/HDS-M05/segmentation/MontgomerySet/CXR_png``\n",
    "- Corresponding masks:\n",
    "    - Left Mask in ``/cdtshared/Cohort-2021/HDS-M05/segmentation/MontgomerySet/ManualMask/leftMask``\n",
    "    - Right Mask in ``/cdtshared/Cohort-2021/HDS-M05/segmentation/MontgomerySet/ManualMask/rightMask``\n",
    "- Data format and size: All are in ``.png`` and of variable size (please resize to 256x256) for your practicals\n",
    "\n",
    "[2] Transform --> Normalise your data - mean and std (e.g., if color then normalise all three channels)\n",
    "e.g., torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "[3] Transform --> Always convert data to ToTensor (you can do **step 1, 2 and 3** in one line as done in this tutorial)\n",
    "\n",
    "[4] New: split your data into **train, validation, and test set**\n",
    "\n",
    "[4] Make [DataLoaders](https://pytorch.org/docs/stable/data.html): It represents a Python iterable over a dataset\n",
    "\n",
    "[5] Identify labels \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms \n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image # For reading your custom data images\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Load data and include prepared transform (Remember to apply same transform to both image and label) \n",
    "class mySegmentationData(object):\n",
    "    def __init__(self, root, transforms = None):\n",
    "        self.root = root\n",
    "        self._eval = eval\n",
    "        self.transforms = transforms\n",
    "        self.build_dataset()\n",
    "        \n",
    "    def build_dataset(self):   \n",
    "        self.imgs = os.path.join(self.root, \"CXR_png\")\n",
    "        self.masks_L = os.path.join(self.root, \"ManualMask/leftMask\")\n",
    "        self.masks_R = os.path.join(self.root, \"ManualMask/rightMask\")\n",
    "        \n",
    "        self._images = glob.glob(self.imgs + \"/*.png\")\n",
    "        \n",
    "#       Note: Here we are using only left lung data today\n",
    "        self._labels = glob.glob(self.masks_L  + \"/*.png\")\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        img = Image.open(self._images[idx]).convert(\"L\").resize((256,256), resample=0)\n",
    "        mask = Image.open(self._labels[idx]).convert(\"L\").resize((256, 256), resample=0)\n",
    "        mask = Image.fromarray(((np.asarray(mask)).astype('float32')).astype('uint8'))\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "            mask = self.transforms(mask)\n",
    "   \n",
    "        return img, mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New: You will split train data into train and val set (say 90-10) \n",
    "- This step is crucial to identify under- and over-fitting problems \n",
    "- Later, we will visualise performance on both train and test online during training (using tensorboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step: Split between train, val, and test from the overall trainset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "transforms.Resize((256,256))\n",
    "trainset = mySegmentationData(root='/cdtshared/Cohort-2021/HDS-M05/segmentation/MontgomerySet/', transforms=transform)\n",
    "\n",
    "#  same as you did in classification (here we are doing )\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "val_percentage = 0.2\n",
    "num_train = len(trainset)\n",
    "\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(val_percentage * num_train))\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "\n",
    "print(len(train_sampler))\n",
    "\n",
    "# Now create data loaders (same as before)\n",
    "# Now we need to create dataLoaders that will allow to iterate during training\n",
    "batch_size = 4 # create batch-based on how much memory you have and your data size\n",
    "\n",
    "traindataloader = DataLoader(trainset, batch_size=batch_size, sampler=train_sampler, num_workers=0)\n",
    "valdataloader = DataLoader(trainset, batch_size=batch_size, sampler=valid_sampler,\n",
    "            num_workers=0,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of training samples:', len(traindataloader))\n",
    "print('Number of validation samples:', len(valdataloader))\n",
    "# print('Number of testing samples:', len(testdataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look into data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to unnormalise images and using transpose to change order to [H, W, Channel] \n",
    "def imshow(img):\n",
    "    # TODO: unnormalize if needed\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "# always check the shape of your training data\n",
    "dataiter = iter(traindataloader)\n",
    "images, masks = dataiter.next()\n",
    "\n",
    "# show images \n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "imshow(torchvision.utils.make_grid(masks))\n",
    "# print labels\n",
    "# print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your UNet model\n",
    "- Please note that your input image size will make difference on your hard-coded feature sizes in FC-layer\n",
    "- Always be aware of what size input is used, here for convenience we will follow the original paper and reshape image to 224x224x3 \n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/EtyQs.png\" style=\"width:800px;height:400px;\">\n",
    "<caption><center> <u>Figure</u>: U-Net architecture for image segmentation.</center></caption>\n",
    "\n",
    "**Hint**\n",
    "- You will make an encoder-decoder U-Net architecture \n",
    "    - Make a baseblock with --> two convolution with kernel_size 3 and padding 1 with relu \n",
    "    - Make another output block with 1 convolution\n",
    "    - For encoder you will use maxpooling with kernel 2\n",
    "    - For decoder you will use upsampling using nn.Upsample (scale_factor=2) or ConvTranspose2d\n",
    "    - Both encoder and decoder will have **baseblock**\n",
    "    \n",
    "\n",
    "- You will learn how to group repeated blocks into one while designing U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic convolution layer -- 2 convs concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Build a nn.Sequential with two conv-BatchN-Relu layers\n",
    "def baseblock(channel_in, channel_out):\n",
    "#     Design a baseblock with conv-batch-relu x 2 (each input is twice convolved as in fig.)\n",
    "    return nn.Sequential(\n",
    "        # your code here\n",
    "        nn.Conv2d(channel_in, channel_out, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(channel_out),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(channel_out, channel_out, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(channel_out),\n",
    "        nn.ReLU(inplace=True)     \n",
    "    )  \n",
    "\n",
    "# Step 2: Build a downscaling module [Hint: use the above layeredConv after that]\n",
    "# Add a maxpool before baseblock as in figure\n",
    "def downsamplePart(channel_in, channel_out):\n",
    "    return nn.Sequential(\n",
    "            #your code,\n",
    "            nn.MaxPool2d(2),\n",
    "            baseblock(channel_in, channel_out)\n",
    "        )\n",
    "\n",
    "# Step 3: Build a upscaling module [Hint: use the above layeredConv after that]\n",
    "# - Remember there is also concatenation and size may change so we are padding\n",
    "class upsampledPart(nn.Module):\n",
    "    def __init__(self, channel_in, channel_out, bilinear=True):\n",
    "        super().__init__()\n",
    "        if bilinear:\n",
    "        self.up = nn.Upsample(scale_factor=2, mode = 'bilinear', align_corners=True)\n",
    "        else: \n",
    "        self.up = nn.ConvTranspose2d(channel_in, channel_out, kernel_size=2, stride=2)\n",
    "        self.conv = baseblock(channel_in + channel_out, channel_out)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        # upscale and then pad to eliminate any difference between upscaled and other feature map coming with skip connection     \n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2] )\n",
    "        \n",
    "        # concatenate (perform concatenation of x1 and x2 --> remember these are skip x2(from encoder) and upssampled image x1)\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        \n",
    "        # apply baseblock after concatenation --> you do again two convs.? --> baseblock \n",
    "\n",
    "        return self.conv(x)\n",
    "    \n",
    "# Step 4: Compile all of above together\n",
    "# here output channel should be equal to number of classes\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, channel_in, channel_out, bilinear=None):\n",
    "        super(UNet,self).__init__()\n",
    "\n",
    "        self.channel_in = channel_in\n",
    "        self.channel_out = channel_out\n",
    "        \n",
    "        #call your base block\n",
    "        self.initial = baseblock(channel_in, 64)\n",
    "        \n",
    "        # downsampling layers with 2 conv layers\n",
    "        self.down1 = downsamplePart(64, 128)\n",
    "        self.down2 = downsamplePart(128, 256)\n",
    "        self.down3 = downsamplePart(256, 512)\n",
    "        self.down4 = downsamplePart(512, 1024)\n",
    "        \n",
    "        # your code here\n",
    "        # upsampling layers with feature concatenation and 2 conv layers \n",
    "        self.up1 = upsampledPart(1024, 512) \n",
    "        self.up2 = upsampledPart(512, 256)\n",
    "        self.up3 = upsampledPart(256, 128)\n",
    "        self.up4 = upsampledart(128, 64)\n",
    "        \n",
    "        # output layer\n",
    "        self.out = nn.Conv2d(64, channel_out, kernel_size=1) \n",
    "\n",
    "    # build a forward pass here\n",
    "    # remember to keep your output as you will need to concatenate later in upscaling\n",
    "    def forward(self,x):\n",
    "        x1 = self.initial(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        # your code here for upscaling\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "\n",
    "        # output\n",
    "        return self.out(x)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare an optimizer, set learning rate, and you loss function\n",
    "- Here you will use model.train and use gradients\n",
    "- Also, you will use criterion to compute loss \n",
    "- Compute metric to know how well it is performing\n",
    "- save them to display mean for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call you model \n",
    "model = UNet(channel_in=1, channel_out= 1)\n",
    "lr = 0.001\n",
    "# optimiser = optim.Adam(model.parameters(), lr=lr)\n",
    "# Optimiser\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr = lr, weight_decay = 1e-8)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# optimiser = torch.optim.RMSprop(model.parameters(), lr = lr, weight_decay = 1e-8, momentum=0.9)\n",
    "# TODO: you can try with different loss function\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare accuracy computation to know how your training is going \n",
    "\n",
    "[1] Loss function is important to keep track (mostly you minimise it, i.e. it should go down)\n",
    "\n",
    "[2] Accuracy in classification is important and you want higher accuracy\n",
    "\n",
    "[3] You can use TensorBoard to visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics\n",
    "def dice(a, b):\n",
    "    \"\"\"Calculate dice score for each image in tensor\"\"\"\n",
    "    # a and b are tensors of shape (B, C, H, W)\n",
    "    # Sum over last two axes (H and W i.e. each image)\n",
    "    return 2*(a*b).sum(axis=[-2, -1])/(a + b).sum(axis=[-2,-1]).type(torch.float32)\n",
    "\n",
    "def mask_out(out):\n",
    "    \"\"\"Mask tensor/array with 0 threshold\"\"\"\n",
    "    # Need to binarize the output to be able to calculate dice score\n",
    "    return out > 0\n",
    "\n",
    "def get_dice_arr(out, label):\n",
    "    \"\"\"Get dice score for each image in the batch for each mask seperately\"\"\"\n",
    "    # Output is shape (B, C)\n",
    "    return dice(mask_out(out), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & validation: same as your classification task!!\n",
    "model.to(device)\n",
    "model.train()\n",
    "# Tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# Writer will output to ./runs/ directory by default\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# define no. of epochs you want to loop \n",
    "epochs = 10\n",
    "log_interval = 100 # for visualising your iterations \n",
    "\n",
    "# New: savining your model depending on your best val score\n",
    "best_valid_loss = float('inf')\n",
    "ckptFileName = 'UNet_CKPT_best.pt'\n",
    "for epoch in range(epochs):\n",
    "    train_loss, valid_loss, train_dsc,val_dsc  = [], [], [], []\n",
    "    \n",
    "    for batch_idx, (data, label) in enumerate(traindataloader):\n",
    "        # initialise all your gradients to zer\n",
    "\n",
    "        label = label.to(device)\n",
    "        optimiser.zero_grad()\n",
    "        out = model(data.to(device))\n",
    "\n",
    "        loss = criterion(out, label)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        # append\n",
    "        train_loss.append(loss.item())\n",
    "        acc_1 = get_dice_arr(out, label.to(device))\n",
    "        train_dsc.append(acc_1.mean(axis=0).detach().cpu().numpy())\n",
    "        \n",
    "        if (batch_idx % log_interval) == 0:\n",
    "            print('Train Epoch is: {}, train loss is: {:.6f} and train dice: {:.6f}'.format(epoch, np.mean(train_loss),np.mean(train_dsc)))\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                for i, (data, label) in enumerate(valdataloader):\n",
    "                    data, label = data.to(device), label.to(device)\n",
    "                    out = model(data)\n",
    "                    loss = criterion(out, label.to(device))\n",
    "                    acc_1 = get_dice_arr(out, label.to(device))\n",
    "\n",
    "                    # append\n",
    "                    val_dsc.append(acc_1.mean(axis=0).detach().cpu().numpy())\n",
    "                    valid_loss.append(loss.item())\n",
    "    \n",
    "            print('Val Epoch is: {}, val loss is: {:.6f} and val dice: {:.6f}'.format(epoch, np.mean(valid_loss), np.mean(val_dsc)))\n",
    "    \n",
    "    # Uncomment it to save your epochs\n",
    "#     if np.mean(valid_loss) < best_valid_loss:\n",
    "#         best_valid_loss = np.mean(valid_loss)\n",
    "#         print('saving my model, improvement in validation loss achieved...')\n",
    "#         torch.save(model.state_dict(), ckptFileName)\n",
    "        \n",
    "        \n",
    "    # every epoch write the loss and accuracy (these you can see plots on tensorboard)        \n",
    "    writer.add_scalar('UNet/train_loss', np.mean(train_loss), epoch)\n",
    "    writer.add_scalar('UNet/train_accuracy', np.mean(train_dsc), epoch)\n",
    "    \n",
    "    # New --> add plot for your val loss and val accuracy\n",
    "    writer.add_scalar('UNet/val_loss', np.mean(valid_loss), epoch)\n",
    "    writer.add_scalar('UNet/val_accuracy', np.mean(val_dsc), epoch)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: compute the accuracy of your model on validation set --> You are required to use dice similarity coefficient\n",
    "total = 0\n",
    "model.eval()\n",
    "dataiter = iter(valdataloader)\n",
    "images, masks = dataiter.next()\n",
    "output = model(images.to(device))\n",
    "# show images \n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "imshow(torchvision.utils.make_grid(output.detach().cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply on your 10% validaton data and report your result\n",
    "ckptFileName = 'UNet_CKPT_best.pt'\n",
    "# load the saved weights\n",
    "model.load_state_dict(torch.load(ckptFileName))\n",
    "# Apply testing (same as validation above)\n",
    "# Your code here... (same as validation code)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving your network peerformance\n",
    "\n",
    "- Train for larger batch size and epochs (longer)\n",
    "- Add data augmentation provided in transforms (https://pytorch.org/vision/stable/transforms.html) -- Ask tutor if you  are confused \n",
    "- Save your training with augmentation as ``my_UNet_withAug.pt``\n",
    "- You can use combined loss funtion as well (e.g. dice loss with BCE loss)\n",
    "- Train your network by combining both left and right lung segmentation (report accuracy and loss curves)\n",
    "\n",
    "\n",
    "#### Exercise: Perform above improvements on  chest X-ray imaging\n",
    "- You can use this ipython notebook to do this (**Assignment to be submmitted**)\n",
    "- Due on **Friday 4th November, 2022 (6.00PM, midday)** (*You will be graded for this exercise*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Thanks for completing this lesson!</h3>\n",
    "The tutorial was written by Dr A. Sharib. (revised by Bartek Papiez, Oct 2022). <br>\n",
    "Any comments or feedbacks and your solution to exercise, please send to bartomiej.papiez@bdi.ox.ac.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
